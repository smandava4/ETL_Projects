Twitter API Pipeline 
    We Use  Python
     Twitter APi:This APi is used as the data source 
      Apache airflow: We have the workflow here : Each unit is a task and DIRECTED ACYCLIC GRAPH(DAG) are used 
      S3 : We store our final transformed  output onto a bucket in s3
      Ec2 : We launch our airflow server in this ec2 instance 

Step1: Get access to Twitter API --> Create an account in twitter 
Step 2: Generate authentication token and API key foe the app
    
    client_id="NXlTTnBaTVA1N0gzYmIzSzVBUEU6MTpjaQ"
    client_secret="adpXK8KMonzmZei4VYr46E0YA8t5DyAsI-E1DH6JJ4d5i9vSNn"

    bearer_token="AAAAAAAAAAAAAAAAAAAAAK6VtgEAAAAAKiGT%2BAUdHmZOaalOhyxaMu%2FauPQ%3DT66Sdf3OjkRetT20cuYnkyhSFk12U8b41JpaFFfyf7aRO9AmeR"

    access_token="1786949785745805312-BUTmZNBevVwenaocVJpokAIhHe8TzQ"
    access_secret="SZ4LRE0pGXQgoC1bBPpfnZQjEWdUixIIXQe2uQe6U7zLv"

    API KEY="IRKuObZ6HMciRlOivWM23Kuec"
    API_KEY_Secret="JaCJgDIg35c9D9f2wt5MairhwGF6kLLc1V3jeHDSyXGGpWIn9W"

    
Step3: Create a python file and install necessary packages 
            pandas 
            tweepy  // package useed to access the twitter data
            s3fs //package to access and store in s3 bucket 

Step 4 : import necessary packages
Step 5 : Store the api key , secret and api key and secret;

Step 6: Establishing a connection between our code and api 